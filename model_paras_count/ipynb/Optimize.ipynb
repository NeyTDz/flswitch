{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e21addb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import collections\n",
    "import os\n",
    "from collections import defaultdict,Counter\n",
    "import tensorflow\n",
    "from tensorflow.keras.utils import *\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten,Dense\n",
    "import hashlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0db73df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = [0,1,2,3,4,5,6,7,8]\n",
    "#调用to_categorical将b按照9个类别来进行转换\n",
    "b = to_categorical(b, 9)\n",
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ae71ab",
   "metadata": {},
   "source": [
    "### Generate Data\n",
    "#### 小规模"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d41500",
   "metadata": {},
   "source": [
    "- 基本参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "770eee40",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 10\n",
    "spacesize = 30\n",
    "n = 15\n",
    "k = 4\n",
    "sparse = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081be61f",
   "metadata": {},
   "source": [
    "- 生成hash值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b0b4c4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3, 17, 27, 23,  3, 12,  2, 14, 15, 21, 18,  8, 16, 11, 15],\n",
       "       [24,  6, 28, 28, 21,  5, 12, 20, 29, 26, 25, 26,  4, 16, 19],\n",
       "       [ 5, 26, 20,  1,  6, 17, 24, 22,  5,  9,  3, 15,  0, 27, 23],\n",
       "       [11, 18, 23, 11, 12, 28, 23, 24,  8, 18, 18, 15, 27, 18,  4],\n",
       "       [24,  2, 11, 17, 18, 22, 11, 21, 23, 25,  1, 28, 28, 11, 15],\n",
       "       [29, 27, 22, 20, 18,  4,  0, 29, 25, 14, 20, 18, 16,  8, 22],\n",
       "       [16,  7, 23,  8,  3, 18,  8, 28, 13, 23,  7, 29,  5, 17,  4],\n",
       "       [ 7,  0, 21, 23, 17, 16, 24,  4, 22,  0, 27, 18, 17, 12,  5],\n",
       "       [ 7, 14, 13, 24, 26,  5,  2,  3, 16,  9,  5,  3, 21,  8,  1],\n",
       "       [18, 28, 18,  9, 15,  9, 21, 10,  8, 13, 12, 15, 15,  6,  2]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hash_list = np.array([np.random.choice(n, spacesize, replace=True) for i in range(m)])\n",
    "hash_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d3388b",
   "metadata": {},
   "source": [
    "#### 真实权重(DNN)\n",
    "- 基本参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b92b4a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 1000\n",
    "n = 20\n",
    "spacesize = 15\n",
    "k = 3\n",
    "sparse = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e91f0f8",
   "metadata": {},
   "source": [
    "- 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9429889c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subdataset(train_num,test_num,random=True):\n",
    "    (train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "    if random:\n",
    "        train_indices = np.random.choice(len(train_images), train_num, replace=False)\n",
    "        test_indices = np.random.choice(len(test_images), test_num, replace=False)\n",
    "        x_train = train_images[train_indices].astype('float32')\n",
    "        y_train = train_labels[train_indices]\n",
    "        x_test = test_images[test_indices].astype('float32')\n",
    "        y_test = test_labels[test_indices] \n",
    "    else:\n",
    "        x_train = train_images[:train_num].astype('float32')\n",
    "        y_train = train_labels[:train_num]\n",
    "        x_test = test_images[:test_num].astype('float32')\n",
    "        y_test = test_labels[:test_num]    \n",
    "    return x_train,y_train,x_test,y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871587fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc1e279c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 0s 719us/step - loss: 4.8773 - accuracy: 0.9510\n",
      "63/63 [==============================] - 0s 697us/step - loss: 4.6896 - accuracy: 0.9555\n",
      "63/63 [==============================] - 0s 712us/step - loss: 3.7390 - accuracy: 0.9660\n",
      "63/63 [==============================] - 0s 681us/step - loss: 3.0559 - accuracy: 0.9665\n",
      "63/63 [==============================] - 0s 725us/step - loss: 3.7779 - accuracy: 0.9540\n",
      "63/63 [==============================] - 0s 712us/step - loss: 5.3732 - accuracy: 0.9535\n",
      "63/63 [==============================] - 0s 697us/step - loss: 4.4714 - accuracy: 0.9475\n",
      "63/63 [==============================] - 0s 712us/step - loss: 5.3836 - accuracy: 0.9435\n",
      "63/63 [==============================] - 0s 650us/step - loss: 3.4090 - accuracy: 0.9570\n",
      "63/63 [==============================] - 0s 697us/step - loss: 4.2272 - accuracy: 0.9570\n",
      "63/63 [==============================] - 0s 776us/step - loss: 3.9649 - accuracy: 0.9550\n",
      "63/63 [==============================] - 0s 681us/step - loss: 4.0296 - accuracy: 0.9520\n",
      "63/63 [==============================] - 0s 697us/step - loss: 4.2861 - accuracy: 0.9560\n",
      "63/63 [==============================] - 0s 669us/step - loss: 3.6223 - accuracy: 0.9590\n",
      "63/63 [==============================] - 0s 760us/step - loss: 5.7957 - accuracy: 0.9465\n",
      "63/63 [==============================] - 0s 681us/step - loss: 4.4090 - accuracy: 0.9545\n",
      "63/63 [==============================] - 0s 681us/step - loss: 4.5540 - accuracy: 0.9480\n",
      "63/63 [==============================] - 0s 649us/step - loss: 3.1077 - accuracy: 0.9705\n",
      "63/63 [==============================] - 0s 637us/step - loss: 5.4962 - accuracy: 0.9440\n",
      "63/63 [==============================] - 0s 681us/step - loss: 4.5674 - accuracy: 0.9520\n"
     ]
    }
   ],
   "source": [
    "clients = n\n",
    "train_num,test_num = 10000,2000\n",
    "hidden = 1024\n",
    "clients_models = []\n",
    "#clients_weights = []\n",
    "#clients_bias = []\n",
    "#clients_hash = []\n",
    "for i in range(clients):\n",
    "    model = Sequential([ \n",
    "          Flatten(input_shape=(28, 28)), \n",
    "          Dense(hidden,'relu'), \n",
    "          Dense(10, \"softmax\"),\n",
    "          ])\n",
    "    model.compile(optimizer='adam', \n",
    "              loss='sparse_categorical_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "    x_train,y_train,x_test,y_test = get_subdataset(train_num,test_num,random=True)\n",
    "    model.fit(x=x_train,y=y_train, epochs=30, verbose = 0)\n",
    "    print(\"Client \",i+1,end=' ')\n",
    "    model.evaluate(x_test, y_test)\n",
    "    clients_models.append(model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2dafce3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_mat = []\n",
    "path = \"../paras/dnn_weights_10240.npy\"\n",
    "for i,model in enumerate(clients_models):\n",
    "    hidden_weights = model.get_weights()[2]\n",
    "    hidden_weights_flat = hidden_weights.reshape(-1)\n",
    "    weights_mat.append(hidden_weights_flat)\n",
    "weights_mat = np.array(weights_mat).T\n",
    "weights_mat = weights_mat\n",
    "np.save(path,weights_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "72f5dbaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden = 128\n",
    "model = Sequential([\n",
    "    Flatten(input_shape=(28,28)),\n",
    "    Dense(hidden,'relu'),\n",
    "    Dense(10,'softmax'),\n",
    "    ])\n",
    "model.load_weights(\"test_weights.h5\")\n",
    "hidden_weights = model.get_weights()[0]\n",
    "#print(hidden_weights.shape)\n",
    "hidden_weights_flat = hidden_weights.reshape(-1)\n",
    "#print(hidden_weights_flat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a29eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_sample = hidden_weights[0]\n",
    "print(weights_sample[:100])\n",
    "head_set = []\n",
    "for w in weights_sample:\n",
    "    we = '{:.5e}'.format(w)\n",
    "    head_set.append(int(we[-1:]))\n",
    "head = min(head,min(head_set))\n",
    "weights_cut = [round(w*(10**head)) for w in weights_sample]\n",
    "print(weights_cut[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23893fa",
   "metadata": {},
   "source": [
    "- 生成hash值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "59e0af07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_hashlist(weights_mat:np.array):\n",
    "    # get the most common power\n",
    "    power_dict = dict()\n",
    "    for weights in weights_mat:\n",
    "        for w in weights:\n",
    "            we = '{:.5e}'.format(w)\n",
    "            power = we[-1:] #power of para in AeB format(B)\n",
    "            power_dict[power] = power_dict[power]+1 if power_dict.get(power) else 1\n",
    "    commond_power = int(max(power_dict.items(),key=lambda x:x[1])[0])\n",
    "    #commond_power = int(max(power_dict.keys()))\n",
    "    # compute the hash value of head and its value space\n",
    "    hash_list = []\n",
    "    max_head, min_head = -float(\"inf\"),float(\"inf\")\n",
    "    for i,weights in enumerate(weights_mat):\n",
    "        weights_head = [round(w*(10**commond_power)) for w in weights]\n",
    "        max_head,min_head = max(max_head,np.max(weights_head)), min(min_head,np.min(weights_head))\n",
    "        hash_list.append([hash(str(wh)) for wh in weights_head])\n",
    "    hash_size = max_head - min_head + 1\n",
    "    return hash_list,hash_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4e49aa",
   "metadata": {},
   "source": [
    "- 统计不重复元素的频数  $O(n)$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e60ce9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stat(h):\n",
    "    stat_dict = defaultdict(list)\n",
    "    for i,s in enumerate(h):\n",
    "        stat_dict[s].append(i)\n",
    "    stat_keys = list(stat_dict.keys())\n",
    "    #stat = [(key,len(stat_dict[key])) for key in stat_keys]\n",
    "    return stat_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "105bf414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 20 58\n"
     ]
    }
   ],
   "source": [
    "para_size = m\n",
    "path = \"./weights/dnn_weights.npy\"\n",
    "weights_mat = np.load(path)\n",
    "weights_mat = weights_mat[:para_size]\n",
    "hash_list, hash_size = generate_hashlist(weights_mat)\n",
    "stat_dict = [get_stat(h) for h in hash_list]\n",
    "print(len(hash_list),len(hash_list[0]),hash_size)\n",
    "#stat_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f648e386",
   "metadata": {},
   "source": [
    "### TopK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12345b57",
   "metadata": {},
   "source": [
    "- 提取频数topk的client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f54955aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def BucketFrequent(k,spacesize,stat):\n",
    "    Bucket = [[] for i in range(spacesize)]\n",
    "    for s in stat:\n",
    "        Bucket[s[1]].append(s)\n",
    "    r = 0\n",
    "    flag = False\n",
    "    result = []\n",
    "    for i in range(len(Bucket)-1,-1,-1):\n",
    "        if not flag and Bucket[i] != []:\n",
    "            for s in Bucket[i]:\n",
    "                result.append(s)\n",
    "                r += 1\n",
    "                if r == k:\n",
    "                    flag = True\n",
    "                    break\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "56d89e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "#取父/子节点基本操作\n",
    "def lchild(node):\n",
    "    return node << 1\n",
    "def rchild(node):\n",
    "    return node << 1 | 1\n",
    "def father(node):\n",
    "    return node >> 1\n",
    "#上浮节点，用于向堆插入新节点\n",
    "def heap_up(heap,node):\n",
    "    val = heap[node]\n",
    "    while father(node)>0 and val[1]<heap[father(node)][1]:\n",
    "        heap[node] = heap[father(node)]\n",
    "        node = father(node)\n",
    "    heap[node] = val\n",
    "#下沉节点，用于调整堆\n",
    "def heap_down(heap,node,k):\n",
    "    root = node #root作为变量逐步下沉\n",
    "    val = heap[node] #存储node原值\n",
    "    while lchild(root) <= k:\n",
    "        child = lchild(root) #先选取左子节点\n",
    "        if rchild(root) <= k and child < rchild(root): #如果右更小，选取右子节点\n",
    "            child = rchild(root)\n",
    "        #验证确实当前节点值大于选取的子节点，则交换\n",
    "        if heap[child][1] < val[1]:\n",
    "            heap[root] = heap[child]\n",
    "            root = child\n",
    "        else: #否则则找到位置，结束循环\n",
    "            break\n",
    "    heap[root] = val #最后赋值\n",
    "#堆排序输出\n",
    "def heap_sort(heap):\n",
    "    for i in range(len(heap)-1, 0, -1):\n",
    "        heap[1], heap[i] = heap[i], heap[1]\n",
    "        heap_down(heap, 1, i)\n",
    "#主函数\n",
    "def HeapFrequent(k,stat):\n",
    "    #stat = collections.Counter(nums)\n",
    "    #stat = list(stat.items())\n",
    "    heap = [(0,0)] #占位上界\n",
    "    #使用heap_up()建堆 规模：k+1(包括占位上界) \n",
    "    for i in range(k):\n",
    "        heap.append(stat[i])\n",
    "        heap_up(heap, len(heap)-1)\n",
    "    #使用heap_down()维护堆 新元素大于堆顶即下沉\n",
    "    for i in range(k,len(stat)):\n",
    "        if stat[i][1] > heap[1][1]: #heap[1]为堆顶\n",
    "            heap[1] = stat[i] #去除原根（堆顶）节点\n",
    "            heap_down(heap,1,k) #将node下沉\n",
    "    #使用heap_sort()排序，并倒序得到从大到小结果\n",
    "    heap_sort(heap)\n",
    "    result = heap[1:][::-1]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e7d37cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def BucketElement(stat_dict,k,spacesize):\n",
    "    stat_keys = list(stat_dict.keys())\n",
    "    stat = [(key,len(stat_dict[key])) for key in stat_keys]\n",
    "    #frequent = BucketFrequent(k,spacesize,stat)\n",
    "    frequent = HeapFrequent(k,stat)\n",
    "    #print(frequent)\n",
    "    elements = [(freq[0],freq[1],stat_dict[freq[0]]) for freq in frequent]\n",
    "    #print(elements)\n",
    "    return frequent,elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "4a08288e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(x,n):\n",
    "    one_hot = np.zeros(n)\n",
    "    for i in x:\n",
    "        one_hot[i] = 1\n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942367ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = para_size\n",
    "n = clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "9fa0acdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 3 clients in each para\n",
      "S_inter [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "Inter Client: 1 contains paras 787\n",
      "Sparse paras: 213\n",
      "[6, 7, 8, 12, 13, 14, 20, 21, 30, 31, 32, 34, 37, 43, 44, 51, 52, 60, 66, 68, 70, 71, 74, 75, 79, 84, 85, 90, 91, 94, 95, 100, 102, 104, 106, 113, 116, 118, 123, 126, 128, 129, 133, 136, 141, 146, 157, 162, 167, 174, 175, 180, 184, 186, 187, 193, 195, 197, 198, 202, 203, 207, 208, 226, 227, 228, 229, 232, 236, 243, 245, 246, 250, 257, 262, 264, 271, 274, 275, 276, 277, 280, 282, 289, 290, 293, 294, 299, 302, 306, 309, 311, 313, 315, 320, 322, 327, 331, 332, 334, 338, 339, 340, 342, 345, 346, 348, 349, 356, 357, 365, 366, 373, 375, 384, 397, 398, 400, 401, 407, 408, 416, 425, 440, 441, 454, 456, 458, 460, 462, 464, 467, 470, 471, 480, 486, 489, 490, 501, 505, 511, 512, 513, 520, 529, 530, 532, 533, 534, 542, 547, 550, 553, 554, 556, 562, 567, 572, 573, 577, 578, 580, 582, 587, 588, 592, 602, 605, 606, 613, 621, 626, 627, 630, 633, 635, 644, 647, 648, 653, 655, 657, 658, 659, 663, 664, 668, 670, 671, 675, 677, 679, 687, 707, 713, 715, 718, 725, 733, 751, 754, 783, 807, 862, 873, 890, 902, 904, 936, 942, 946, 954, 968]\n",
      "Sparse percent 0.213\n"
     ]
    }
   ],
   "source": [
    "Sp = []\n",
    "Lp = []\n",
    "print(\"Top\",k,\"clients in each para\")\n",
    "for std in stat_dict:\n",
    "    frequents,elements = BucketElement(std,k,spacesize)\n",
    "    #print(\"F\",frequents,elements)\n",
    "    select = []\n",
    "    l = 0\n",
    "    for i in range(len(elements)):\n",
    "        select += elements[i][2]\n",
    "        l += elements[i][1]\n",
    "    select = one_hot(select, n)\n",
    "    #print(select)\n",
    "    Sp.append(select)\n",
    "    Lp.append(l)\n",
    "Sp = np.mat(Sp)\n",
    "# 根据大小排序\n",
    "Lpindex = sorted(range(len(Lp)), key=lambda k: Lp[k], reverse=True)\n",
    "Sp = Sp[Lpindex]\n",
    "# 求交集\n",
    "S_inter = np.ones((1,n))\n",
    "B_inter = []\n",
    "B_sparse = []\n",
    "for i,s in enumerate(Sp):\n",
    "    s_mat = np.diag(s.tolist()[0])\n",
    "    flag = np.matmul(S_inter,s.T)\n",
    "    if flag > 0:\n",
    "        S_inter = np.matmul(S_inter,s_mat)\n",
    "        B_inter.append(i)\n",
    "    else:\n",
    "        B_sparse.append(i)\n",
    "print(\"S_inter\",S_inter)\n",
    "print(\"Inter Client:\",np.argmax(S_inter),\"contains paras\",len(B_inter))\n",
    "print(\"Sparse paras:\",len(B_sparse))\n",
    "print(B_sparse)\n",
    "print(\"Sparse percent\",len(B_sparse) / m)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8 (default, Apr 13 2021, 15:08:03) [MSC v.1916 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0e53523e0924413c28b9a2923d8adcf153adbfd5ac500c47cf42b69b5eb267f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
